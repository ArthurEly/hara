import pandas as pd
import numpy as np
import os
import re
import ast
from datetime import datetime
import shutil

# Fun√ß√µes auxiliares (mantidas como no original)
def is_listlike_string(val):
    return isinstance(val, str) and val.startswith("[") and val.endswith("]")

def clean_listlike_column(val):
    try:
        if is_listlike_string(val):
            lst = ast.literal_eval(val)
            if isinstance(lst, list) and all(isinstance(x, (int, float)) for x in lst):
                return np.prod(lst)
    except:
        pass
    return val

def expand_listlike_columns(df):
    cols_to_expand = [col for col in df.columns if df[col].apply(is_listlike_string).any()]
    for col in cols_to_expand:
        new_col_name = f"{col} (dimensions flattened)"
        idx = df.columns.get_loc(col)
        expanded_series = df[col].apply(clean_listlike_column)
        df.insert(idx + 1, new_col_name, expanded_series)
    
    df.drop(columns=cols_to_expand, inplace=True)
    return df

def extract_bitwidth(x):
    x_str = str(x).upper()
    if "B'BINARY" in x_str:
        return 1
    match = re.search(r'(UINT|INT)(\d+)', x_str)
    return int(match.group(2)) if match else None

# Constantes de configura√ß√£o
area_cols_to_preserve = [
    "Total LUTs", "LUTRAMs", "Logic LUTs", "FFs", "RAMB36", "RAMB18", "DSP Blocks",
    "ConvKernelDim", "Dilation", "Stride", "SRLs", "inWidth", "Padding", "K",
    "Labels", "PE", "inFIFODepths", "outFIFODepths", "numInputVectors",
]

def clean_dataframe(df_input):
    df = df_input.copy()

    # Colunas a serem preservadas de certas etapas de limpeza
    existing_area_cols = [col for col in area_cols_to_preserve if col in df.columns]
    cols_to_preserve_current_df = list(existing_area_cols)

    # Inserir colunas "(bits)" a partir de colunas de tipo de dados
    datatype_cols = [col for col in df.columns if "datatype" in col.lower()]
    generated_bit_cols = []
    for col in datatype_cols:
        new_col_name = f"{col} (bits)"
        generated_bit_cols.append(new_col_name)
        bitwidths = df[col].apply(extract_bitwidth)
        col_idx = df.columns.get_loc(col)
        df.insert(col_idx + 1, new_col_name, bitwidths)
    
    cols_to_preserve_current_df.extend(generated_bit_cols)
    df.drop(columns=datatype_cols, inplace=True, errors='ignore')

    # 1. Remover colunas com valor √∫nico
    nunique = df.nunique()
    cols_to_drop_unique = [col for col in nunique[nunique <= 1].index if col not in cols_to_preserve_current_df]
    df.drop(columns=cols_to_drop_unique, inplace=True, errors='ignore')

    # 2. Tentar converter para num√©rico (movido para o final)

    # 3. Remover colunas altamente correlacionadas (apenas num√©ricas)
    numeric_df = df.select_dtypes(include=np.number)
    cols_to_drop_corr = []
    if not numeric_df.empty and numeric_df.shape[1] > 1:
        corr_matrix = numeric_df.corr().abs()
        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
        cols_to_drop_corr = [
            column for column in upper.columns
            if any(upper[column] > 0.98) and column not in cols_to_preserve_current_df
        ]
        if cols_to_drop_corr:
            print(f"  Removendo colunas altamente correlacionadas: {cols_to_drop_corr}")
        df.drop(columns=cols_to_drop_corr, inplace=True, errors='ignore')

    # 4. Remover duplicatas
    df.drop_duplicates(inplace=True)

    # 5. L√≥gica de remo√ß√£o de colunas com valores ausentes foi desativada
    cols_to_drop_missing = [] 

    # 6. L√≥gica de remo√ß√£o de colunas espec√≠ficas foi desativada

    # 7. Renomear colunas IFM/OFM
    rename_map = {
        col: col.replace("IFM", "Input Feature Map ").replace("OFM", "Output Feature Map ")
        for col in df.columns
        if "IFM" in col or "OFM" in col
    }
    df.rename(columns=rename_map, inplace=True)

    # 8. Expandir colunas com valores list-like
    df = expand_listlike_columns(df)

    # --- IN√çCIO DA L√ìGICA DE ONE-HOT ENCODING (COM MANUTEN√á√ÉO DE ORDEM) ---
    # 9. Aplicar One-Hot Encoding em colunas categ√≥ricas, ignorando as especificadas
    
    cols_to_ignore_encoding = ["Hardware config", "Submodule Instance"]
    all_object_cols = df.select_dtypes(include=['object']).columns
    categorical_cols_to_encode = [col for col in all_object_cols if col not in cols_to_ignore_encoding]
    
    if categorical_cols_to_encode:
        print(f"  Aplicando One-Hot Encoding com preserva√ß√£o de ordem nas colunas: {categorical_cols_to_encode}")

        for cat_col in categorical_cols_to_encode:
            # 1. Salvar a posi√ß√£o original da coluna
            original_idx = df.columns.get_loc(cat_col)

            # 2. Gerar as colunas dummy para a coluna categ√≥rica atual
            #    Adicionado dtype=int para garantir valores 0/1 em vez de True/False
            dummies = pd.get_dummies(df[cat_col], prefix=cat_col, prefix_sep='_', dtype=int)

            # 3. Renomear as colunas dummy para o formato "isColunaValor"
            rename_map = {}
            for dummy_col in dummies.columns:
                value = dummy_col.split(f"{cat_col}_", 1)[1]
                clean_value = ''.join(e for e in str(value) if e.isalnum()).capitalize()
                new_name = f"is{cat_col.capitalize()}{clean_value}"
                rename_map[dummy_col] = new_name
            dummies.rename(columns=rename_map, inplace=True)
            
            # 4. Remover a coluna categ√≥rica original do DataFrame
            df.drop(columns=[cat_col], inplace=True)

            # 5. Inserir as novas colunas na posi√ß√£o original
            cols_before = df.columns[:original_idx]
            cols_after = df.columns[original_idx:]
            
            df = pd.concat([df[cols_before], dummies, df[cols_after]], axis=1)

        print(f"  Colunas categ√≥ricas transformadas e reordenadas.")
    # --- FIM DA L√ìGICA DE ONE-HOT ENCODING ---

    # 10. Tentar converter todas as colunas restantes para num√©rico (se poss√≠vel)
    for col in df.columns:
        if col not in cols_to_ignore_encoding:
            df[col] = pd.to_numeric(df[col], errors='ignore')

    stats = {
        "original_columns": len(nunique),
        "removed_constant": len(cols_to_drop_unique),
        "removed_high_corr": len(cols_to_drop_corr),
        "removed_missing": len(cols_to_drop_missing),
        "categorical_encoded": len(categorical_cols_to_encode),
        "final_columns": df.shape[1]
    }
    return df, stats

def main_pipeline():
    project_root_dir = "." 
    scenario_roots = [
        os.path.join(project_root_dir, "results/complete"),
        os.path.join(project_root_dir, "results/splitted")
    ]
    print(f"Execu√ß√£o do pipeline de pr√©-processamento iniciada.\n")
    for scenario_input_dir in scenario_roots:
        scenario_name = os.path.basename(scenario_input_dir)
        print(f"--- Processando Cen√°rio: {scenario_name.upper()} ---")
        abs_scenario_input_dir = os.path.abspath(scenario_input_dir)
        print(f"Diret√≥rio de entrada: {abs_scenario_input_dir}")
        if not os.path.isdir(abs_scenario_input_dir):
            print(f"‚ö†Ô∏è  Aviso: Diret√≥rio de entrada n√£o encontrado: {abs_scenario_input_dir}. Pulando cen√°rio.\n")
            continue
        try:
            input_csv_files = [f for f in os.listdir(abs_scenario_input_dir) if os.path.isfile(os.path.join(abs_scenario_input_dir, f)) and f.endswith(".csv")]
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  Aviso: Erro ao listar arquivos em {abs_scenario_input_dir}. Pulando cen√°rio.\n")
            continue
        if not input_csv_files:
            print(f"‚ÑπÔ∏è  Nenhum arquivo CSV encontrado em {abs_scenario_input_dir}. Pulando cen√°rio.\n")
            continue
        print(f"Encontrados {len(input_csv_files)} arquivo(s) CSV para processar: {', '.join(input_csv_files)}")
        output_dir = os.path.join(abs_scenario_input_dir, "preprocessed")
        os.makedirs(output_dir, exist_ok=True) 
        print(f"Diret√≥rio de sa√≠da: {output_dir}")
        scenario_cleaned_dfs = {}
        scenario_summary_stats = {}
        for csv_file in input_csv_files:
            file_path = os.path.join(abs_scenario_input_dir, csv_file)
            df_name_key = csv_file.replace(".csv", "") 
            print(f"\n  Processando arquivo: {csv_file}...")
            try:
                original_df = pd.read_csv(file_path)
                cleaned_df, processing_stats = clean_dataframe(original_df)
                scenario_cleaned_dfs[df_name_key] = cleaned_df
                scenario_summary_stats[df_name_key] = processing_stats
                print(f"  Arquivo '{csv_file}' limpo com sucesso.")
            except Exception as e:
                print(f"  ‚ùå Erro ao processar o arquivo {csv_file}: {e}")
        if not scenario_cleaned_dfs:
            print(f"‚ÑπÔ∏è  Nenhum DataFrame foi limpo com sucesso para o cen√°rio '{scenario_name}'. Opera√ß√µes de salvamento puladas.\n")
            continue
        # Atualizando o summary_df para incluir a nova estat√≠stica
        summary_df = pd.DataFrame(scenario_summary_stats).T
        summary_df = summary_df.reset_index().rename(columns={'index': 'OriginalFile'})
        # Reordenar colunas do sum√°rio para melhor visualiza√ß√£o
        summary_cols_order = ['OriginalFile', 'original_columns', 'removed_constant', 'removed_high_corr', 'removed_missing', 'categorical_encoded', 'final_columns']
        summary_df = summary_df[[col for col in summary_cols_order if col in summary_df.columns]]
        
        for df_key, df_content in scenario_cleaned_dfs.items():
            cleaned_output_filename = f"{df_key}_cleaned.csv"
            full_output_df_path = os.path.join(output_dir, cleaned_output_filename)
            try:
                df_content.to_csv(full_output_df_path, index=False)
                print(f"  ‚úÖ Salvo: {full_output_df_path}")
            except Exception as e:
                print(f"  ‚ùå Erro ao salvar {cleaned_output_filename} em {output_dir}: {e}")
        
        summary_output_filename = "summary_preprocessing.csv"
        full_output_summary_path = os.path.join(output_dir, summary_output_filename)
        try:
            summary_df.to_csv(full_output_summary_path, index=False)
            print(f"  üìù Resumo salvo: {full_output_summary_path}")
        except Exception as e:
            print(f"  ‚ùå Erro ao salvar {summary_output_filename} em {output_dir}: {e}")
        print(f"--- Cen√°rio {scenario_name.upper()} finalizado ---\n")

    print("‚úÖ Pipeline de pr√©-processamento conclu√≠do para todos os cen√°rios.")

if __name__ == '__main__':
    main_pipeline()